{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6403afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Yakina/.cache/kagglehub/datasets/hsankesara/flickr-image-dataset/versions/1/flickr30k_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f647d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv \n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = path + 'flickr30k_images/'\n",
    "csv_path = path + 'results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8804d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_number = 2 # or None\n",
    "features_file = 'image_features.h5'\n",
    "\n",
    "model_file = f'models/model_epoch_{saved_model_number}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_path, sep = r'\\s*\\|\\s*')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['image_name'] = data['image_name'].astype(str)\n",
    "data['comment_number'] = data['comment_number'].astype(int)\n",
    "data['comment'] = data['comment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20736b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    data['comment'] = data['comment'].apply(lambda x: x.lower())\n",
    "    data['comment'] = data['comment'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
    "    data['comment'] = data['comment'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
    "    data['comment'] = data['comment'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
    "    data['comment'] = \"startseq \" + data['comment'] + \" endseq\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b696c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text_preprocessing(data)\n",
    "captions = data['comment'].tolist()\n",
    "captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(caption.split()) for caption in captions)\n",
    "\n",
    "images = data['image_name'].unique().tolist()\n",
    "nimages = len(images)\n",
    "\n",
    "split_index = round(0.85*nimages)\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]\n",
    "\n",
    "train = data[data['image_name'].isin(train_images)]\n",
    "test = data[data['image_name'].isin(val_images)]\n",
    "\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "tokenizer.texts_to_sequences([captions[1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, vocab_size, max_length, features,shuffle=True):\n",
    "    \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.features = features\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "    \n",
    "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n",
    "        X1, X2, y = self.__get_data(batch)        \n",
    "        return (X1, X2), y\n",
    "    \n",
    "    def __get_data(self,batch):\n",
    "        \n",
    "        X1, X2, y = list(), list(), list()\n",
    "        \n",
    "        images = batch[self.X_col].tolist()\n",
    "           \n",
    "        for image in images:\n",
    "            feature = self.features[image][0]\n",
    "            \n",
    "            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                for i in range(1,len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            \n",
    "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                \n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd187208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_caption_model_architecture():\n",
    "\tinput1 = Input(shape=(1920,))\n",
    "\tinput2 = Input(shape=(max_length,))\n",
    "\n",
    "\timg_features = Dense(256, activation='relu')(input1)\n",
    "\timg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "\tsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
    "\tmerged = concatenate([img_features_reshaped,sentence_features],axis=1)\n",
    "\tsentence_features = LSTM(256)(merged)\n",
    "\tx = Dropout(0.5)(sentence_features)\n",
    "\tx = add([x, img_features])\n",
    "\tx = Dense(128, activation='relu')(x)\n",
    "\tx = Dropout(0.5)(x)\n",
    "\toutput = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "\tcaption_model = Model(inputs=[input1,input2], outputs=output)\n",
    "\n",
    "\tcaption_model.compile(loss='categorical_crossentropy', \n",
    "\t\t\t\t\t\toptimizer='adam', \n",
    "\t\t\t\t\t\tmetrics=['accuracy'])\n",
    "\t\n",
    "\treturn caption_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img_features():\n",
    "\tmodel = DenseNet201()\n",
    "\tfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "\timg_size = 224\n",
    "\tfeatures = {}\n",
    "\tfor image in tqdm(data['image_name'].unique().tolist()):\n",
    "\t\timg = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n",
    "\t\timg = img_to_array(img)\n",
    "\t\timg = img/255.\n",
    "\t\timg = np.expand_dims(img,axis=0)\n",
    "\t\tfeature = fe.predict(img, verbose=0)\n",
    "\t\tfeatures[image] = feature\n",
    "\n",
    "\twith h5py.File(features_file, 'w') as f:\n",
    "\t\tfor img_name, feature_array in features.items():\n",
    "\t\t\tf.create_dataset(img_name, data = feature_array)\n",
    "\t\n",
    "\tprint(f\"Features saved to {features_file}\")\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if saved_model_number == 0:\n",
    "\tfeatures = extract_img_features()\n",
    "\tcaption_model = create_caption_model_architecture()\n",
    "else:\n",
    "\tfeatures = {}\n",
    "\twith h5py.File(features_file, 'r') as f:\n",
    "\t\tfor img_name in f.keys():\n",
    "\t\t\tfeatures[img_name] = f[img_name][:]\n",
    "\tprint(f\"Features loaded from {features_file}\")\n",
    "\tcaption_model = load_model(model_file)\n",
    "\tprint(f\"Model loaded from {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5239cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(df = train, X_col = 'image_name', y_col = 'comment', batch_size = 64,directory = image_path, tokenizer = tokenizer, vocab_size = vocab_size, max_length = max_length, features = features)\n",
    "\n",
    "validation_generator = CustomDataGenerator(df = test, X_col = 'image_name', y_col = 'comment', batch_size = 64, directory = image_path,  tokenizer = tokenizer, vocab_size = vocab_size, max_length = max_length, features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7114072",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_epoch_{epoch:02d}.h5\"\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                            monitor = \"val_loss\",\n",
    "                            mode = \"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose = 1)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                                            patience = 3, \n",
    "                                            verbose = 1, \n",
    "                                            factor = 0.2, \n",
    "                                            min_lr = 0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfdbfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_file=\"training_log.csv\"):\n",
    "        super().__init__()\n",
    "        self.log_file = log_file\n",
    "        \n",
    "        if not os.path.exists(log_file):\n",
    "            with open(log_file, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['timestamp', 'epoch', 'loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        \n",
    "        with open(self.log_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                epoch + 1,  # Epoch number (1-indexed)\n",
    "                logs.get('loss', 'N/A'),\n",
    "                logs.get('accuracy', 'N/A'),\n",
    "                logs.get('val_loss', 'N/A'),\n",
    "                logs.get('val_accuracy', 'N/A'),\n",
    "                lr\n",
    "            ])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Loss: {logs.get('loss', 'N/A'):.4f}\")\n",
    "        print(f\"  Accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"  Val Loss: {logs.get('val_loss', 'N/A'):.4f}\")\n",
    "        print(f\"  Val Accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"  Learning Rate: {lr:.8f}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ad57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_logger = MetricsLogger(\"training_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = caption_model.fit(\n",
    "        train_generator,\n",
    "        epochs = 50,\n",
    "        validation_data = validation_generator,\n",
    "        callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n",
    "        initial_epoch = saved_model_number,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
